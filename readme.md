# Network Security Data - End-to-End Machine Learning Project

## Table of Contents
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Pipeline Components](#pipeline-components)
  - [Data Ingestion](#data-ingestion)
  - [Data Validation](#data-validation)
  - [Data Transformation](#data-transformation)
  - [Model Training](#model-training)
- [MLFlow & Dagshub](#mlflow--dagshub)
- [FastAPI Interface](#fastapi-interface)
- [Setup](#setup)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Overview
This project is an end-to-end Machine Learning pipeline designed for Network Security Data analysis. The pipeline involves data ingestion from a MongoDB database, data validation, transformation, model training, evaluation, and model deployment. The objective is to predict network security events using a trained machine learning model.

Key components used in this project:
- **Database**: MongoDB
- **Experiment Tracking**: MLFlow
- **Remote Repository**: Dagshub
- **Web Interface**: FastAPI

## Project Structure
Here is a high-level view of the project structure:

![Project Structure](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/project_structure.png)

```bash
ðŸ“‚ network-security-ml-project
â”œâ”€â”€ ðŸ“‚ Artifacts                       # Directory for artifacts generated during training runs
â”‚   â”œâ”€â”€ ðŸ“‚ 11_20_2024_00_36_05         # Artifacts with timestamp (YYYY_MM_DD_HH_MM_SS)
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ data_ingestion          # This directory contains the ingested data from MongoDB
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ feature_store
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ phisingData.csv    # Ingested data full 
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“‚ ingested
â”‚   â”‚   â”‚       â”œâ”€â”€ test.csv          # Ingested test data
â”‚   â”‚   â”‚       â””â”€â”€ train.csv         # Ingested train data
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ data_transformation     # This directory contains the transformed ingested data in numpy array format and preprocessor object.
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ transformed
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ test.npy          # Tranformed test data
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ train.npy         # Tranformed train data
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“‚ transformed_object
â”‚   â”‚   â”‚       â””â”€â”€ preprocessing.pkl    # Preprocessing pipeline object
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ data_validation        # This directory contains the validated data and validation reports.
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ drift_report    
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ report.yaml      # Drift report of ingested data compared to previous data
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“‚ validated          # Folder containing validated data
â”‚   â”‚   â”‚       â”œâ”€â”€ test.csv          # Validated test data
â”‚   â”‚   â”‚       â””â”€â”€ train.csv          # Validated train data
â”‚   â”‚   â””â”€â”€ ðŸ“‚ model_trainer          # Directory containing the trained Machine Learning model
â”‚   â”‚       â””â”€â”€ ðŸ“‚ trained_model
â”‚   â”‚           â””â”€â”€ model.pkl        # Trained Machine Learning Model
â”‚   â”œâ”€â”€ ðŸ“‚ 11_21_2024_11_54_42         # Another timestamped directory for artifacts
â”‚   â”‚   â””â”€â”€ (similar structure as above)
â”‚   â”œâ”€â”€ ðŸ“‚ 11_21_2024_12_20_13
â”‚   â”‚   â””â”€â”€ (similar structure as above)
â”‚   â”œâ”€â”€ ðŸ“‚ 11_21_2024_13_54_19
â”‚   â”‚   â””â”€â”€ (similar structure as above)
â”‚   â””â”€â”€ ðŸ“‚ 11_21_2024_13_59_57
â”‚       â””â”€â”€ (similar structure as above)
â”œâ”€â”€ ðŸ“œ Dockerfile                      # Docker configuration file for containerizing the application
â”œâ”€â”€ ðŸ“‚ NetworkSecurity.egg-info        # Metadata directory for the Python package
â”‚   â”œâ”€â”€ PKG-INFO
â”‚   â”œâ”€â”€ SOURCES.txt
â”‚   â”œâ”€â”€ dependency_links.txt
â”‚   â”œâ”€â”€ requires.txt
â”‚   â””â”€â”€ top_level.txt
â”œâ”€â”€ ðŸ“œ app.py                          # Script for deploying a web application (using FastAPI)
â”œâ”€â”€ ðŸ“‚ final_model                     # Directory for storing the final trained model and preprocessing object.
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ preprocessor.pkl
â”œâ”€â”€ ðŸ“‚ logs                            # Logs generated during different runs for monitoring
â”œâ”€â”€ ðŸ“œ main.py                         # Main entry point to run the pipeline or trigger components
â”œâ”€â”€ ðŸ“‚ network_data                    # Folder to store network-related datasets
â”‚   â””â”€â”€ phisingData.csv
â”œâ”€â”€ ðŸ“‚ network_security                # Main package for the project containing source code
â”‚   â”œâ”€â”€ ðŸ“œ __init__.py                 # Logging configuration and utilities
â”‚   â”œâ”€â”€ ðŸ“‚ cloud                       # Code related to cloud operations (Not yet implemented)
â”‚   â”‚   â””â”€â”€ ðŸ“œ __init__.py
â”‚   â”œâ”€â”€ ðŸ“‚ components                  # Code for each component of the pipeline
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â””â”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ ðŸ“‚ constants                   # Constants used throughout the project
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â””â”€â”€ ðŸ“‚ training_pipeline
â”‚   â”‚       â””â”€â”€ ðŸ“œ __init__.py
â”‚   â”œâ”€â”€ ðŸ“‚ entity                      # Data entities and configuration entities for the pipeline
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â”‚   â””â”€â”€ config_entity.py
â”‚   â”œâ”€â”€ ðŸ“‚ exception                   # Custom exception handling for the project
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â””â”€â”€ exception.py
â”‚   â”œâ”€â”€ ðŸ“‚ logging                     
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ ðŸ“‚ pipeline                    # Pipeline scripts for batch prediction and training
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch_prediction.py
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â””â”€â”€ ðŸ“‚ utils                       # Utility functions for common tasks
â”‚       â”œâ”€â”€ ðŸ“œ __init__.py
â”‚       â”œâ”€â”€ common.py
â”‚       â””â”€â”€ ðŸ“‚ model                   # Model-related utilities (e.g., saving, loading)
â”‚           â”œâ”€â”€ ðŸ“œ __init__.py
â”‚           â””â”€â”€ estimator.py
â”œâ”€â”€ ðŸ“‚ notebooks                       # Jupyter notebooks for EDA, model experiments, etc.
â”œâ”€â”€ ðŸ“‚ prediction_output               # Outputs from model predictions.
â”‚   â””â”€â”€ output.csv
â”œâ”€â”€ ðŸ“œ push_data.py                    # Script to push data to a database or storage system
â”œâ”€â”€ ðŸ“œ readme.md                       # Project README file
â”œâ”€â”€ ðŸ“œ requirements.txt                # List of dependencies for the project
â”œâ”€â”€ ðŸ“‚ research                        # Research and experimentation notebooks
â”‚   â””â”€â”€ data_ingestion.ipynb
â”œâ”€â”€ ðŸ“œ schema.yaml                     # Schema definition for validation of datasets
â”œâ”€â”€ ðŸ“œ setup.py                        # Script to install the package
â”œâ”€â”€ ðŸ“‚ templates                       # HTML templates for web app(FastAPI)
â”‚   â””â”€â”€ table.html
â”œâ”€â”€ ðŸ“œ test_mongodb.py                 # Script for testing MongoDB connections
â””â”€â”€ ðŸ“‚ valida_data                     # Folder for testing project working on FastAPI web app.
    â””â”€â”€ test.csv

```

# Pipeline Components

## Data Ingestion

The Data Ingestion phase is crucial for setting up the initial dataset required for the Machine Learning workflow. This step ingests data from a MongoDB database and structures it for further processing. Below is a breakdown of the Data Ingestion process:

![Data Ingestion](https://your-image-url-here)

#### Key Steps:

1. **Configuration:**
   - The **Data Ingestion Config** specifies various paths:
     - **Data Ingestion Directory**: Directory where the ingested data will be stored.
     - **Feature Store File Path**: Path for the feature store which holds the raw data.
     - **Training and Testing File Paths**: Paths where training and testing datasets will be saved.
     - **Collection Name**: Name of the MongoDB collection from which data will be ingested.
     - **Train-Test Split Ratio**: Defines the proportion of data allocated for training versus testing.
2. **Initiate Data Ingestion:**
   - The process begins by calling the data ingestion function, which connects to the specified MongoDB database.

3. **Export Data to Feature Store:**
   - The data is exported to the feature store as a CSV file. This is the raw data and serves as the baseline for all subsequent processes.
4. **Data Ingestion Artifact:**
   - Finally, a **Data Ingestion Artifact** is created to maintain metadata about the ingestion process, including timestamps and paths to the ingested files.

#### Output:
- The end result of the Data Ingestion process is the creation of:
  - **Feature Store** containing the raw dataset as CSV.
  - **Ingested Data** folders containing `train.csv` and `test.csv` files for subsequent processing.

This structured approach ensures that the data flow into the pipeline is efficient, clean, and well-documented, providing a strong foundation for further stages of the data processing pipeline.

## Data Validation

The Data Validation phase ensures that the ingested data meets the necessary quality standards before proceeding to the next steps in the Machine Learning pipeline. The following diagram outlines the key aspects of this process:

![Data Validation](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/data_validation.png)

### Overview of the Process:

1. **Configuration**:
   - The process starts with a **Data Validation Config** that defines directories for valid and invalid data, as well as paths for drift reports.

2. **Initiate Validation**:
   - Data validation is initiated, starting with reading the ingested CSV files (`train.csv` and `test.csv`).

3. **Column Validation**:
   - The number of columns and their data types is validated against the predefined schema. This includes checks to ensure the correct columns are present and that numerical columns exist.

4. **Validation Status**:
   - The validation status is recorded, indicating whether any columns are missing or if there are issues with the data types.

5. **Dataset Drift Detection**:
   - If the initial validation passes, the process checks for dataset drift, which helps ensure that the data remains consistent with the training parameters.

6. **Data Validation Artifact**:
   - Finally, a **Data Validation Artifact** is created, summarizing the validation status and producing a drift report in JSON format.


## Data Transformation

The Data Transformation phase prepares the validated data for modeling by handling missing values and scaling features. The following diagram illustrates the key steps in this process:

![Data Transformation](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/data_transformation.png)

### Overview of the Process:

1. **Configuration**:
   - The process begins with a **Data Transformation Config**, which sets the parameters for the transformation pipeline.

2. **Initiate Data Transformation**:
   - The data transformation process is initiated, starting with the reading of the validated CSV files (`train.csv` and `test.csv`).

3. **Feature Handling**:
   - Missing values in the dataset are handled using techniques such as Robust Scaling and KNN Imputer. The target column is dropped from the training data to prepare for transformation.

4. **Data Preparation**:
   - The training and testing data are converted into feature arrays, preparing for input into the ML model. Techniques like SMOTE are applied to address imbalances in the data.

5. **Concatenation and Finalization**:
   - The final transformed training and testing arrays are created and concatenated accordingly.

6. **Artifact Creation**:
   - The transformation includes saving a preprocessor object (`preprocessing.pkl`) and the resulting numpy arrays (`train.npy` and `test.npy`) as artifacts for future use.


## Model Training

The Model Training phase is vital for building and optimizing a machine learning model based on the transformed data. The following diagram illustrates the core steps involved in this process:

![Model Training](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/model_trainer.png)

### Overview of the Process:

1. **Configuration**:
   - The process begins with a **Model Trainer Config** that defines parameters such as model file paths and expected accuracy.

2. **Initiate Model Training**:
   - Model training is initiated by loading the transformed numpy arrays (`train.npy` and `test.npy`) for further processing.

3. **Data Preparation**:
   - The training and testing arrays are prepared by splitting the features and target values, ensuring the data is structured for model input.

4. **Model Selection**:
   - Multiple models are evaluated, including:
     - Random Forest
     - Decision Tree
     - Gradient Boosting
     - Logistic Regression
     - AdaBoost
   - GridSearchCV is utilized for hyperparameter tuning to find the best combination of parameters for each model.

5. **Model Evaluation**:
   - The best-performing model is selected based on evaluation metrics, including the best score compared to the expected accuracy.

6. **Artifact Creation**:
   - After training, the best model is saved as an object (`model.pkl`) along with associated metrics, which are logged for future reference.

## MLFlow & Dagshub

In this project, MLFlow and Dagshub are utilized for effective tracking of experiments, model performance, and version control. These tools enhance the reproducibility and maintainability of the machine learning workflow.

### MLFlow

MLFlow is an open-source platform designed for managing the machine learning lifecycle. It provides various components including:

- **Experiment Tracking**: MLFlow allows tracking of model training runs, parameters, metrics, and artifacts. Each experiment is recorded, making it easy to compare model performance over time.
- **Model Repository**: Models can be logged and later retrieved for deployment or further analysis. The built-in tracking server stores all experiment details in one place.

### Key Features of MLFlow in this Project:
- Logging of hyperparameters and metrics during model training.
- Storage of trained model files for easy retrieval.
- Visualization tools for comparing different runs and determining the best-performing model.

### Dagshub

Dagshub serves as a remote repository tailored for data science and machine learning projects. It integrates well with Git and supports versioning for datasets, code, and model files.

### Key Features of Dagshub in this Project:
- **Version Control**: Facilitates tracking changes in datasets and code, allowing team collaboration and versioning for reproducibility.
- **Experiment Management**: Dagshub provides a platform for storing and comparing different experiments, complements MLFlowâ€™s capabilities by preserving all components of the workflow in a single location.

### Integration
In this project, both MLFlow and Dagshub are configured to work together, ensuring that every aspect of the training and evaluation process is logged and easily accessible. This integration enhances project transparency and allows for effective collaboration among team members.



## FastAPI Interface

The FastAPI framework is used to create a simple web interface for making predictions based on the trained machine learning model. This interface allows users to input data and receive predictions, facilitating easy interaction with the model.

### Key Features:
- **RESTful API**: FastAPI provides a RESTful interface to interact with the model, making it accessible for various applications.
- **Data Validation**: Incoming requests are validated automatically, ensuring that the data format aligns with the expected schema.
- **Asynchronous Support**: FastAPI leverages asynchronous programming, improving performance for handling multiple requests.

### How to Run:
To start the FastAPI application, run the following command in your terminal:
```bash
uvicorn app:app --reload
```
This will start the server locally, and you can access the interface at `http://127.0.0.1:8000/docs` to view the available endpoints.

---

## Setup

To set up the environment for this project, follow the steps below:

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-username/network-security-ml-project.git
   cd network-security-ml-project
   ```

2. **Install Dependencies**:
   Use the provided `requirements.txt` file to install the necessary packages:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set Up MongoDB**:
   Ensure that you have a MongoDB instance running and accessible. Update the connection details in the configuration files as necessary.

4. **Environment Variables**:
   If applicable, set up any required environment variables for the project.

---

## Usage

Once the setup is complete, you can use the project as follows:

1. **Push Data To MongoDB**:
   Push data to MongoDB by running the push_data script:
   ```bash
   python push_data.py
   ```


2. **Execute main pipeline**:
   Execute the main.py file to run the pipeline and all its components:
   ```bash
   python main.py
   ```

3. **Run the FastAPI Application**:
   Start the FastAPI server to make predictions:
   ```bash
   uvicorn app:app --reload
   ```

Once the server is running, you can send POST requests to the prediction endpoint to receive outputs based on input data.
