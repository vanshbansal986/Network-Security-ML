# Network Security Data - End-to-End Machine Learning Project

## Table of Contents
- [Overview](#overview)
- [Project Structure](#project-structure)
- [Pipeline Components](#pipeline-components)
  - [Data Ingestion](#data-ingestion)
  - [Data Validation](#data-validation)
  - [Data Transformation](#data-transformation)
  - [Model Training](#model-training)
  - [Model Evaluation](#model-evaluation)
  - [Model Pushing](#model-pushing)
- [MLFlow & Dagshub](#mlflow--dagshub)
- [FastAPI Interface](#fastapi-interface)
- [Setup](#setup)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Overview
This project is an end-to-end Machine Learning pipeline designed for Network Security Data analysis. The pipeline involves data ingestion from a MongoDB database, data validation, transformation, model training, evaluation, and model deployment. The objective is to predict network security events using a trained machine learning model.

Key components used in this project:
- **Database**: MongoDB
- **Experiment Tracking**: MLFlow
- **Remote Repository**: Dagshub
- **Web Interface**: FastAPI

## Project Structure
Here is a high-level view of the project structure:

![Project Structure](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/project_structure.png)

```bash
ðŸ“‚ network-security-ml-project
â”œâ”€â”€ ðŸ“‚ Artifacts                       # Directory for artifacts generated during training runs
â”‚   â”œâ”€â”€ ðŸ“‚ 11_20_2024_00_36_05         # Artifacts with timestamp (YYYY_MM_DD_HH_MM_SS)
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ data_ingestion          # This directory contains the ingested data from MongoDB
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ feature_store
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ phisingData.csv    # Ingested data full 
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“‚ ingested
â”‚   â”‚   â”‚       â”œâ”€â”€ test.csv          # Ingested test data
â”‚   â”‚   â”‚       â””â”€â”€ train.csv         # Ingested train data
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ data_transformation     # This directory contains the transformed ingested data in numpy array format and preprocessor object.
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ transformed
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ test.npy          # Tranformed test data
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ train.npy         # Tranformed train data
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“‚ transformed_object
â”‚   â”‚   â”‚       â””â”€â”€ preprocessing.pkl    # Preprocessing pipeline object
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ data_validation        # This directory contains the validated data and validation reports.
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ drift_report    
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ report.yaml      # Drift report of ingested data compared to previous data
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“‚ validated          # Folder containing validated data
â”‚   â”‚   â”‚       â”œâ”€â”€ test.csv          # Validated test data
â”‚   â”‚   â”‚       â””â”€â”€ train.csv          # Validated train data
â”‚   â”‚   â””â”€â”€ ðŸ“‚ model_trainer          # Directory containing the trained Machine Learning model
â”‚   â”‚       â””â”€â”€ ðŸ“‚ trained_model
â”‚   â”‚           â””â”€â”€ model.pkl        # Trained Machine Learning Model
â”‚   â”œâ”€â”€ ðŸ“‚ 11_21_2024_11_54_42         # Another timestamped directory for artifacts
â”‚   â”‚   â””â”€â”€ (similar structure as above)
â”‚   â”œâ”€â”€ ðŸ“‚ 11_21_2024_12_20_13
â”‚   â”‚   â””â”€â”€ (similar structure as above)
â”‚   â”œâ”€â”€ ðŸ“‚ 11_21_2024_13_54_19
â”‚   â”‚   â””â”€â”€ (similar structure as above)
â”‚   â””â”€â”€ ðŸ“‚ 11_21_2024_13_59_57
â”‚       â””â”€â”€ (similar structure as above)
â”œâ”€â”€ ðŸ“œ Dockerfile                      # Docker configuration file for containerizing the application
â”œâ”€â”€ ðŸ“‚ NetworkSecurity.egg-info        # Metadata directory for the Python package
â”‚   â”œâ”€â”€ PKG-INFO
â”‚   â”œâ”€â”€ SOURCES.txt
â”‚   â”œâ”€â”€ dependency_links.txt
â”‚   â”œâ”€â”€ requires.txt
â”‚   â””â”€â”€ top_level.txt
â”œâ”€â”€ ðŸ“œ app.py                          # Script for deploying a web application (using FastAPI)
â”œâ”€â”€ ðŸ“‚ final_model                     # Directory for storing the final trained model and preprocessing object.
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ preprocessor.pkl
â”œâ”€â”€ ðŸ“‚ logs                            # Logs generated during different runs for monitoring
â”œâ”€â”€ ðŸ“œ main.py                         # Main entry point to run the pipeline or trigger components
â”œâ”€â”€ ðŸ“‚ network_data                    # Folder to store network-related datasets
â”‚   â””â”€â”€ phisingData.csv
â”œâ”€â”€ ðŸ“‚ network_security                # Main package for the project containing source code
â”‚   â”œâ”€â”€ ðŸ“œ __init__.py                 # Logging configuration and utilities
â”‚   â”œâ”€â”€ ðŸ“‚ cloud                       # Code related to cloud operations (Not yet implemented)
â”‚   â”‚   â””â”€â”€ ðŸ“œ __init__.py
â”‚   â”œâ”€â”€ ðŸ“‚ components                  # Code for each component of the pipeline
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â””â”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ ðŸ“‚ constants                   # Constants used throughout the project
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â””â”€â”€ ðŸ“‚ training_pipeline
â”‚   â”‚       â””â”€â”€ ðŸ“œ __init__.py
â”‚   â”œâ”€â”€ ðŸ“‚ entity                      # Data entities and configuration entities for the pipeline
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â”‚   â””â”€â”€ config_entity.py
â”‚   â”œâ”€â”€ ðŸ“‚ exception                   # Custom exception handling for the project
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â””â”€â”€ exception.py
â”‚   â”œâ”€â”€ ðŸ“‚ logging                     
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ ðŸ“‚ pipeline                    # Pipeline scripts for batch prediction and training
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch_prediction.py
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â””â”€â”€ ðŸ“‚ utils                       # Utility functions for common tasks
â”‚       â”œâ”€â”€ ðŸ“œ __init__.py
â”‚       â”œâ”€â”€ common.py
â”‚       â””â”€â”€ ðŸ“‚ model                   # Model-related utilities (e.g., saving, loading)
â”‚           â”œâ”€â”€ ðŸ“œ __init__.py
â”‚           â””â”€â”€ estimator.py
â”œâ”€â”€ ðŸ“‚ notebooks                       # Jupyter notebooks for EDA, model experiments, etc.
â”œâ”€â”€ ðŸ“‚ prediction_output               # Outputs from model predictions.
â”‚   â””â”€â”€ output.csv
â”œâ”€â”€ ðŸ“œ push_data.py                    # Script to push data to a database or storage system
â”œâ”€â”€ ðŸ“œ readme.md                       # Project README file
â”œâ”€â”€ ðŸ“œ requirements.txt                # List of dependencies for the project
â”œâ”€â”€ ðŸ“‚ research                        # Research and experimentation notebooks
â”‚   â””â”€â”€ data_ingestion.ipynb
â”œâ”€â”€ ðŸ“œ schema.yaml                     # Schema definition for validation of datasets
â”œâ”€â”€ ðŸ“œ setup.py                        # Script to install the package
â”œâ”€â”€ ðŸ“‚ templates                       # HTML templates for web app(FastAPI)
â”‚   â””â”€â”€ table.html
â”œâ”€â”€ ðŸ“œ test_mongodb.py                 # Script for testing MongoDB connections
â””â”€â”€ ðŸ“‚ valida_data                     # Folder for testing project working on FastAPI web app.
    â””â”€â”€ test.csv

```

# Pipeline Components

## Data Ingestion

The Data Ingestion phase is crucial for setting up the initial dataset required for the Machine Learning workflow. This step ingests data from a MongoDB database and structures it for further processing. Below is a breakdown of the Data Ingestion process:

![Data Ingestion](https://your-image-url-here)

#### Key Steps:

1. **Configuration:**
   - The **Data Ingestion Config** specifies various paths:
     - **Data Ingestion Directory**: Directory where the ingested data will be stored.
     - **Feature Store File Path**: Path for the feature store which holds the raw data.
     - **Training and Testing File Paths**: Paths where training and testing datasets will be saved.
     - **Collection Name**: Name of the MongoDB collection from which data will be ingested.
     - **Train-Test Split Ratio**: Defines the proportion of data allocated for training versus testing.
2. **Initiate Data Ingestion:**
   - The process begins by calling the data ingestion function, which connects to the specified MongoDB database.

3. **Export Data to Feature Store:**
   - The data is exported to the feature store as a CSV file. This is the raw data and serves as the baseline for all subsequent processes.
4. **Data Ingestion Artifact:**
   - Finally, a **Data Ingestion Artifact** is created to maintain metadata about the ingestion process, including timestamps and paths to the ingested files.

#### Output:
- The end result of the Data Ingestion process is the creation of:
  - **Feature Store** containing the raw dataset as CSV.
  - **Ingested Data** folders containing `train.csv` and `test.csv` files for subsequent processing.

This structured approach ensures that the data flow into the pipeline is efficient, clean, and well-documented, providing a strong foundation for further stages of the data processing pipeline.

## Data Validation

The Data Validation phase ensures that the ingested data meets the necessary quality standards before proceeding to the next steps in the Machine Learning pipeline. The following diagram outlines the key aspects of this process:

![Data Validation](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/data_validation.png)

### Overview of the Process:

1. **Configuration**:
   - The process starts with a **Data Validation Config** that defines directories for valid and invalid data, as well as paths for drift reports.

2. **Initiate Validation**:
   - Data validation is initiated, starting with reading the ingested CSV files (`train.csv` and `test.csv`).

3. **Column Validation**:
   - The number of columns and their data types is validated against the predefined schema. This includes checks to ensure the correct columns are present and that numerical columns exist.

4. **Validation Status**:
   - The validation status is recorded, indicating whether any columns are missing or if there are issues with the data types.

5. **Dataset Drift Detection**:
   - If the initial validation passes, the process checks for dataset drift, which helps ensure that the data remains consistent with the training parameters.

6. **Data Validation Artifact**:
   - Finally, a **Data Validation Artifact** is created, summarizing the validation status and producing a drift report in JSON format.


## Data Transformation

The Data Transformation phase prepares the validated data for modeling by handling missing values and scaling features. The following diagram illustrates the key steps in this process:

![Data Transformation](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/data_transformation.png)

### Overview of the Process:

1. **Configuration**:
   - The process begins with a **Data Transformation Config**, which sets the parameters for the transformation pipeline.

2. **Initiate Data Transformation**:
   - The data transformation process is initiated, starting with the reading of the validated CSV files (`train.csv` and `test.csv`).

3. **Feature Handling**:
   - Missing values in the dataset are handled using techniques such as Robust Scaling and KNN Imputer. The target column is dropped from the training data to prepare for transformation.

4. **Data Preparation**:
   - The training and testing data are converted into feature arrays, preparing for input into the ML model. Techniques like SMOTE are applied to address imbalances in the data.

5. **Concatenation and Finalization**:
   - The final transformed training and testing arrays are created and concatenated accordingly.

6. **Artifact Creation**:
   - The transformation includes saving a preprocessor object (`preprocessing.pkl`) and the resulting numpy arrays (`train.npy` and `test.npy`) as artifacts for future use.


## Model Training

The Model Training phase is vital for building and optimizing a machine learning model based on the transformed data. The following diagram illustrates the core steps involved in this process:

![Model Training](https://github.com/vanshbansal986/Network-Security-ML/blob/main/images2/model_trainer.png)

### Overview of the Process:

1. **Configuration**:
   - The process begins with a **Model Trainer Config** that defines parameters such as model file paths and expected accuracy.

2. **Initiate Model Training**:
   - Model training is initiated by loading the transformed numpy arrays (`train.npy` and `test.npy`) for further processing.

3. **Data Preparation**:
   - The training and testing arrays are prepared by splitting the features and target values, ensuring the data is structured for model input.

4. **Model Selection**:
   - Multiple models are evaluated, including:
     - Random Forest
     - Decision Tree
     - Gradient Boosting
     - Logistic Regression
     - AdaBoost
   - GridSearchCV is utilized for hyperparameter tuning to find the best combination of parameters for each model.

5. **Model Evaluation**:
   - The best-performing model is selected based on evaluation metrics, including the best score compared to the expected accuracy.

6. **Artifact Creation**:
   - After training, the best model is saved as an object (`model.pkl`) along with associated metrics, which are logged for future reference.
